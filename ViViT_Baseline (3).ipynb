{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XjBUAISQpyMl"
      },
      "outputs": [],
      "source": [
        "# Requiremments installation\n",
        "\n",
        "!pip install --quiet torch  timm==0.9.16 einops accelerate scikit-learn torchaudio torchvision\n",
        "!pip install --quiet transformers evaluate decord 'git+https://github.com/facebookresearch/pytorchvideo.git'\n",
        "\n",
        "import torch, os, math, random, time, json, shutil, glob\n",
        "from pathlib import Path\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "id": "7tGIQZv_qD_W"
      },
      "outputs": [],
      "source": [
        "# Model Configuration\n",
        "# Replace dataset_root with a path toward split dataset\n",
        "\n",
        "cfg = {\n",
        "    \"dataset_root\": \"/content/drive/MyDrive/CADAR Attack Video Dataset\",\n",
        "    \"clip_duration\": 13,\n",
        "    \"frames_per_clip\": 8,\n",
        "    \"batch_size\": 16,\n",
        "    \"num_workers\": 0,\n",
        "    \"epochs\": 10,\n",
        "    \"base_lr\":  1e-4,\n",
        "    \"weight_decay\": 5e-3,\n",
        "    \"num_classes\": 5,\n",
        "}\n",
        "\n",
        "# Note: CUDA must be available to handle training\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "def get_class_counts (root):\n",
        "    class_dirs = [d for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))]\n",
        "    counts = []\n",
        "    for cls in sorted (class_dirs): # keep label order deterministi\n",
        "      n = 0\n",
        "      for ext in ('mp4', 'avi', 'mov'):\n",
        "        n += len(glob.glob(os.path.join(root, cls, f'* {ext}')))\n",
        "      counts.append(n)\n",
        "    return counts\n",
        "class_counts = get_class_counts(os.path.join(cfg [\"dataset_root\"], \"train\"))"
      ],
      "metadata": {
        "id": "0hz0XoCNCjrt"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, random\n",
        "import torch, numpy as np\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pytorchvideo.data.encoded_video import EncodedVideo\n",
        "from pytorchvideo.transforms import UniformTemporalSubsample\n",
        "from torchvision.transforms import Resize\n",
        "\n",
        "# Video transformation\n",
        "def make_video_transform():\n",
        "    resize = Resize((400, 400))\n",
        "    mean   = torch.tensor([0.45,0.45,0.45]).view(3,1,1,1)\n",
        "    std    = torch.tensor([0.225,0.225,0.225]).view(3,1,1,1)\n",
        "\n",
        "    def video_transform(x: torch.Tensor):\n",
        "        if x.ndim==4 and x.shape[-1]==3:\n",
        "            x = x.permute(3,0,1,2)\n",
        "        elif x.ndim==4 and x.shape[1]==3 and x.shape[0]!=3:\n",
        "            x = x.permute(1,0,2,3)\n",
        "\n",
        "        x = UniformTemporalSubsample(cfg[\"frames_per_clip\"])(x)\n",
        "        x = x[:, :2]\n",
        "\n",
        "        C,T,H,W = x.shape\n",
        "        x = x.contiguous().to(torch.float32) / 255.0\n",
        "        x = torch.nn.functional.interpolate(\n",
        "            x.view(C*T,1,H,W),\n",
        "            size=(400,400),\n",
        "            mode=\"bilinear\",\n",
        "            align_corners=False\n",
        "        ).view(C,T,400,400)\n",
        "        x = (x - mean.to(x.device)) / std.to(x.device)\n",
        "\n",
        "        return x.permute(1,0,2,3).contiguous()\n",
        "\n",
        "    return video_transform\n",
        "\n",
        "video_transform = make_video_transform()\n",
        "\n",
        "# Collects 1 clip per video for test and validation set\n",
        "class FirstClipDataset(Dataset):\n",
        "    def __init__(self, labeled_videos, clip_duration, transform):\n",
        "        self.data = labeled_videos\n",
        "        self.clip_duration = clip_duration\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, meta = self.data[idx]\n",
        "        label = meta[\"label\"]\n",
        "        ev = EncodedVideo.from_path(path)\n",
        "        clip = ev.get_clip(0.0, self.clip_duration)\n",
        "        frames = clip[\"video\"]\n",
        "        frames = self.transform(frames)\n",
        "        return {\n",
        "            \"video\":      frames,\n",
        "            \"label\":      torch.tensor(label, dtype=torch.long),\n",
        "            \"video_name\": Path(path).name\n",
        "        }\n",
        "\n",
        "# Returns final dataloader\n",
        "def make_loader(split):\n",
        "    split_dir = os.path.join(cfg[\"dataset_root\"], split)\n",
        "    class_names = sorted(d for d in os.listdir(split_dir)\n",
        "                         if os.path.isdir(os.path.join(split_dir, d)))\n",
        "    class_to_idx = {c:i for i,c in enumerate(class_names)}\n",
        "\n",
        "    pool = []\n",
        "    for cls in class_names:\n",
        "        cls_dir = os.path.join(split_dir, cls)\n",
        "        for ext in (\".mp4\",\".avi\",\".mov\"):\n",
        "            for fp in glob.glob(os.path.join(cls_dir,f\"*{ext}\")):\n",
        "                pool.append((fp, {\"label\": class_to_idx[cls]}))\n",
        "\n",
        "    if split==\"train\":\n",
        "        lbls = [m[\"label\"] for _,m in pool]\n",
        "        counts = np.bincount(lbls, minlength=len(class_names))\n",
        "        w_cls  = 1.0/(counts+1e-6)\n",
        "        w_samp = np.array([w_cls[l] for l in lbls])\n",
        "        p_samp = w_samp / w_samp.sum()\n",
        "        N = len(pool)*2\n",
        "        idxs = np.random.choice(len(pool), size=N, p=p_samp)\n",
        "        videos = [pool[i] for i in idxs]\n",
        "        random.shuffle(videos)\n",
        "    else:\n",
        "        videos = pool\n",
        "\n",
        "    ds = FirstClipDataset(\n",
        "        labeled_videos = videos,\n",
        "        clip_duration  = cfg[\"clip_duration\"],\n",
        "        transform      = video_transform\n",
        "    )\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size  = cfg[\"batch_size\"],\n",
        "        shuffle     = (split==\"train\"),\n",
        "        num_workers = cfg[\"num_workers\"],\n",
        "        pin_memory  = True,\n",
        "    )\n",
        "\n",
        "train_loader = make_loader(\"train\")\n",
        "val_loader   = make_loader(\"val\")\n",
        "test_loader  = make_loader(\"test\")"
      ],
      "metadata": {
        "id": "QbSM7EN-huah"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xnZVM2naqRuB"
      },
      "outputs": [],
      "source": [
        "# Build ViViT model\n",
        "\n",
        "from transformers import VivitForVideoClassification\n",
        "from torch.optim import AdamW\n",
        "from accelerate import Accelerator\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "model = VivitForVideoClassification.from_pretrained(\n",
        "    \"google/vivit-b-16x2-kinetics400\",\n",
        "    ignore_mismatched_sizes=True,\n",
        "    num_labels = cfg[\"num_classes\"],\n",
        "    torch_dtype = torch.float32\n",
        ").to(device)\n",
        "\n",
        "acc = Accelerator(mixed_precision=\"fp16\")\n",
        "\n",
        "# Build finite weight vector\n",
        "eps      = 1e-6\n",
        "counts   = torch.tensor(class_counts, dtype=torch.float32)\n",
        "weights  = 1.0 / (counts + eps)\n",
        "weights  = weights / weights.sum()\n",
        "\n",
        "# Focal loss method\n",
        "class FocalLoss(torch.nn.Module):\n",
        "    def __init__(self, gamma=2.0, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "    def forward(self, logits, targets):\n",
        "        ce  = torch.nn.functional.cross_entropy(logits, targets, reduction='none')\n",
        "        pt  = torch.exp(-ce)\n",
        "        fl  = (1 - pt) ** self.gamma * ce\n",
        "        return fl.mean() if self.reduction == 'mean' else fl.sum()\n",
        "\n",
        "criterion = FocalLoss(gamma=2.0).to(device)\n",
        "\n",
        "# Build optimizer, scheduler\n",
        "optimizer  = AdamW(model.parameters(), lr=cfg[\"base_lr\"],\n",
        "                   weight_decay=cfg[\"weight_decay\"])\n",
        "\n",
        "from transformers import get_cosine_schedule_with_warmup\n",
        "import math\n",
        "\n",
        "def steps_per_epoch(loader, batch_size):\n",
        "    try:\n",
        "        return len(loader)\n",
        "    except TypeError:\n",
        "        ds = loader.dataset\n",
        "        for attr in (\"_labeled_videos\", \"labeled_videos\"):\n",
        "            if hasattr(ds, attr):\n",
        "                n_samples = len(getattr(ds, attr))\n",
        "                return math.ceil(n_samples / batch_size)\n",
        "        raise RuntimeError(\"Cannot infer dataset size for warmup calc.\")\n",
        "\n",
        "sp_epoch = steps_per_epoch(train_loader, cfg[\"batch_size\"])\n",
        "total_steps = sp_epoch * cfg[\"epochs\"]\n",
        "warmup_steps = int(0.1 * total_steps)\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps,\n",
        ")\n",
        "\n",
        "model, optimizer, scheduler = acc.prepare(model, optimizer, scheduler)\n",
        "scaler = GradScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plyOSj0I6id1"
      },
      "outputs": [],
      "source": [
        "# Class accuracy computation for main training loop\n",
        "\n",
        "def compute_per_class_acc(y_true, y_pred, n_classes):\n",
        "    correct_per_class = np.zeros(n_classes, dtype=np.int32)\n",
        "    total_per_class   = np.zeros(n_classes, dtype=np.int32)\n",
        "    for true, pred in zip(y_true, y_pred):\n",
        "        total_per_class[true] += 1\n",
        "        if true == pred:\n",
        "            correct_per_class[true] += 1\n",
        "    return [(correct_per_class[i] / total_per_class[i] if total_per_class[i] > 0 else 0.0)\n",
        "                     for i in range(n_classes)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5L_ho4z8qYUM"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "from math import ceil\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import textwrap, torch\n",
        "\n",
        "def run_epoch(loader, *, train=True, debug=False):\n",
        "\n",
        "    model.train(train) if train else model.eval()\n",
        "\n",
        "    epoch_loss, correct_tot, seen_tot = 0.0, 0, 0\n",
        "    per_class_correct = np.zeros(cfg[\"num_classes\"], dtype=np.int64)\n",
        "    per_class_seen    = np.zeros(cfg[\"num_classes\"], dtype=np.int64)\n",
        "    all_true, all_pred = [], []\n",
        "\n",
        "    try:\n",
        "        total_batches = len(loader)\n",
        "    except TypeError:\n",
        "        total_batches = ceil(len(loader.dataset._labeled_videos) / loader.batch_size)\n",
        "\n",
        "    loop = tqdm(loader, desc=\"Train\" if train else \"Val\",\n",
        "                total=total_batches, leave=False)\n",
        "\n",
        "    for b_idx, batch in enumerate(loop):\n",
        "        vids   = batch[\"video\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        if vids.shape[2] == 1:\n",
        "            vids = vids.repeat(1, 1, 3, 1, 1)\n",
        "\n",
        "        if train:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            logits = model(vids, interpolate_pos_encoding=True).logits\n",
        "            loss   = criterion(logits, labels)\n",
        "\n",
        "        if train:\n",
        "            acc.backward(loss)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        # Performance metrics computation\n",
        "        preds = logits.argmax(1)\n",
        "        bs    = labels.size(0)\n",
        "\n",
        "        epoch_loss   += loss.item() * bs\n",
        "        correct_tot  += (preds == labels).sum().item()\n",
        "        seen_tot     += bs\n",
        "\n",
        "        for c in range(cfg[\"num_classes\"]):\n",
        "            mask = (labels == c)\n",
        "            per_class_seen[c]    += mask.sum().item()\n",
        "            per_class_correct[c] += (preds[mask] == c).sum().item()\n",
        "\n",
        "        all_true.extend(labels.cpu().tolist())\n",
        "        all_pred.extend(preds.cpu().tolist())\n",
        "\n",
        "        overall_acc = correct_tot / seen_tot\n",
        "        class_accs  = [ per_class_correct[i] / per_class_seen[i]\n",
        "                        if per_class_seen[i] else 0\n",
        "                        for i in range(cfg[\"num_classes\"]) ]\n",
        "\n",
        "        post = {\"loss\": f\"{loss.item():.4f}\", \"all\": f\"{overall_acc:.3f}\"}\n",
        "        for i, a in enumerate(class_accs):\n",
        "            post[f\"C{i}\"] = f\"{a:.3f}\"\n",
        "        loop.set_postfix(post)\n",
        "\n",
        "    # Epoch aggregates\n",
        "    epoch_loss  /= seen_tot\n",
        "    overall_acc  = correct_tot / seen_tot\n",
        "    class_accs   = [ per_class_correct[i] / per_class_seen[i]\n",
        "                     if per_class_seen[i] else 0\n",
        "                     for i in range(cfg[\"num_classes\"]) ]\n",
        "\n",
        "    return epoch_loss, overall_acc, class_accs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKgqvtoEqaEJ"
      },
      "outputs": [],
      "source": [
        "# Main training loop\n",
        "use_amp=True\n",
        "\n",
        "for epoch in range(cfg[\"epochs\"]):\n",
        "    tr_loss, tr_acc, tr_class_acc = run_epoch(train_loader, train=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        vl_loss, vl_acc, vl_class_acc = run_epoch(val_loader, train=False)\n",
        "\n",
        "    # Optional metrics output\n",
        "    print(f\"Epoch {epoch+1}/{cfg['epochs']}\")\n",
        "    print(\"  Train loss {:.4f}  overall acc {:.3f}\".format(tr_loss, tr_acc))\n",
        "    print(\"  Val   loss {:.4f}  overall acc {:.3f}\".format(vl_loss, vl_acc))\n",
        "    print(\"  Per‑class train acc:\", [\"{:.3f}\".format(a) for a in tr_class_acc])\n",
        "    print(\"  Per‑class val acc:\", \" \".join([f\"C{i}={a:.3f}\" for i, a in enumerate(vl_class_acc)]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lL6sFZpUqcB8"
      },
      "outputs": [],
      "source": [
        "# Test set performance evaluation\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "with torch.no_grad(), torch.amp.autocast(\"cuda\", enabled=use_cuda):\n",
        "    for batch in test_loader:\n",
        "        vids   = batch[\"video\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        logits = model(vids, interpolate_pos_encoding=True).logits\n",
        "\n",
        "        all_preds.append(logits.argmax(1).cpu())\n",
        "        all_labels.append(labels.cpu())\n",
        "\n",
        "y_pred  = torch.cat(all_preds).numpy()\n",
        "y_true  = torch.cat(all_labels).numpy()\n",
        "\n",
        "prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "    y_true, y_pred, labels=np.arange(5), zero_division=0\n",
        ")\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=np.arange(5))\n",
        "N  = cm.sum()\n",
        "tp = np.diag(cm)\n",
        "fp = cm.sum(axis=0) - tp\n",
        "fn = cm.sum(axis=1) - tp\n",
        "tn = N - tp - fp - fn\n",
        "acc = (tp + tn) / N\n",
        "\n",
        "class_names = [\"Removal\", \"No Attack\", \"Visual Modification\", \"Text Modification\", \"Addition\"]\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"Attack Class\": class_names,\n",
        "    \"Accuracy\":  acc,\n",
        "    \"Precision\": prec,\n",
        "    \"F1 Score\":  f1,\n",
        "})\n",
        "\n",
        "pd.set_option(\"display.precision\", 5)\n",
        "print(\"\\nPer‑class metrics (for Table 2):\")\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "print(\"\\nDetailed classification report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_names, digits=5))\n",
        "\n",
        "print(\"\\nConfusion matrix:\")\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional model download\n",
        "\n",
        "OUT_PATH = \"\" # Drive path\n",
        "\n",
        "torch.save(model.state_dict(), OUT_PATH)\n",
        "\n",
        "# Optional local download\n",
        "from google.colab import files\n",
        "files.download(OUT_PATH)"
      ],
      "metadata": {
        "id": "pc0joPYME0XM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}